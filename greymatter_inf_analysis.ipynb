{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd, numpy as np, lightgbm as lgb, shap\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_parquet(\"influencer_modelling_ready.parquet\")\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\ndf.head()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create ROI calculation with available columns\nQ, E = df[\"Quality_Audience\"], df[\"ER\"]\nM = df[\"Turkey\"]/100\nP = df[\"Est_Post_Price\"]\n\n# Use Comment_Rate as CM proxy (influence multiplier)\nCM = df[\"Comment_Rate\"]\nIM = 1.0  # Placeholder for influence multiplier\n\n# Calculate ROI_star\ndf[\"ROI_star\"] = (Q * E * M * IM * (1 + 0.2*CM)) / P\n\n# Standardize for modeling\ny = (df[\"ROI_star\"] - df[\"ROI_star\"].mean()) / df[\"ROI_star\"].std()\n\nprint(f\"ROI_star stats: mean={df['ROI_star'].mean():.2f}, std={df['ROI_star'].std():.2f}\")\nprint(f\"Target y stats: mean={y.mean():.2f}, std={y.std():.2f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ---------- TRAIN / VALIDATION SPLIT ----------\n# Remove ROI_star from features (don't want to predict using target)\nfeature_cols = [col for col in df.columns if col != \"ROI_star\"]\nX = df[feature_cols]\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# ---------- LIGHTGBM REGRESSOR ----------\nreg = lgb.LGBMRegressor(\n    objective=\"regression\", \n    metric=\"rmse\",\n    learning_rate=0.03, \n    n_estimators=800,\n    num_leaves=128, \n    min_data_in_leaf=10,\n    subsample=0.8, \n    colsample_bytree=0.8,\n    random_state=42,\n    verbose=-1  # Suppress training output\n)\n\n# Train model\nreg.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[lgb.log_evaluation(period=0)]  # Suppress eval logs\n)\n\nprint(f\"Training score: {reg.score(X_train, y_train):.4f}\")\nprint(f\"Validation score: {reg.score(X_val, y_val):.4f}\")\n\n# ---------- SHAP ANALYSIS ----------\ntry:\n    explainer = shap.TreeExplainer(reg)\n    shap_values = explainer.shap_values(X_val.iloc[:100])  # Use subset for speed\n    \n    plt.figure(figsize=(10, 8))\n    shap.summary_plot(shap_values, X_val.iloc[:100], max_display=25, show=False)\n    plt.tight_layout()\n    plt.show()\nexcept Exception as e:\n    print(f\"SHAP analysis failed: {e}\")\n    print(\"Showing feature importance instead:\")\n    \n    # Fallback to feature importance\n    importance = reg.feature_importances_\n    features = X.columns\n    \n    # Sort and display top features\n    feature_imp = pd.DataFrame({\n        'feature': features,\n        'importance': importance\n    }).sort_values('importance', ascending=False)\n    \n    print(\"\\nTop 15 Feature Importances:\")\n    print(feature_imp.head(15))"
  },
  {
   "cell_type": "code",
   "source": "# ---------- STATISTICAL SIGNIFICANCE TESTS ----------\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\n\nprint(\"=\" * 60)\nprint(\"STATISTICAL SIGNIFICANCE ANALYSIS\")\nprint(\"=\" * 60)\n\n# 1. Model Performance Significance Tests\ny_pred = reg.predict(X_val)\nresiduals = y_val - y_pred\n\n# Test if residuals are normally distributed (assumption for many tests)\nshapiro_stat, shapiro_p = stats.shapiro(residuals[:5000] if len(residuals) > 5000 else residuals)\nprint(f\"\\n1. RESIDUAL NORMALITY TEST (Shapiro-Wilk):\")\nprint(f\"   Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4e}\")\nprint(f\"   Residuals {'ARE' if shapiro_p > 0.05 else 'ARE NOT'} normally distributed (α=0.05)\")\n\n# Test if model predictions are significantly different from random\nrandom_pred = np.random.normal(y_val.mean(), y_val.std(), len(y_val))\nt_stat, t_p = stats.ttest_ind(y_pred, random_pred)\nprint(f\"\\n2. MODEL vs RANDOM PREDICTIONS (T-test):\")\nprint(f\"   T-statistic: {t_stat:.4f}, p-value: {t_p:.4e}\")\nprint(f\"   Model predictions {'ARE' if t_p < 0.05 else 'ARE NOT'} significantly different from random (α=0.05)\")\n\n# 2. Feature-Target Correlation Significance Tests\nprint(f\"\\n3. FEATURE-TARGET CORRELATION TESTS:\")\nprint(\"   Top 10 features with significant correlations:\")\n\ncorrelations = []\nfor col in X.columns:\n    if X[col].dtype in ['int64', 'float64']:\n        corr_coef, corr_p = stats.pearsonr(X[col], y)\n        correlations.append({\n            'feature': col,\n            'correlation': corr_coef,\n            'p_value': corr_p,\n            'significant': corr_p < 0.05\n        })\n\ncorr_df = pd.DataFrame(correlations).sort_values('correlation', key=abs, ascending=False)\nprint(corr_df.head(10).to_string(index=False))\n\n# 3. Feature Importance Permutation Test\nprint(f\"\\n4. PERMUTATION IMPORTANCE TEST:\")\nprint(\"   Testing if feature importance is statistically significant...\")\n\nperm_importance = permutation_importance(\n    reg, X_val, y_val, n_repeats=10, random_state=42, scoring='r2'\n)\n\n# Create dataframe with permutation results\nperm_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance_mean': perm_importance.importances_mean,\n    'importance_std': perm_importance.importances_std\n})\n\n# Calculate z-scores for importance (assuming normal distribution)\nperm_df['z_score'] = perm_df['importance_mean'] / (perm_df['importance_std'] + 1e-10)  # Add small epsilon to avoid division by zero\nperm_df['p_value'] = 2 * (1 - stats.norm.cdf(abs(perm_df['z_score'])))\nperm_df['significant'] = perm_df['p_value'] < 0.05\nperm_df = perm_df.sort_values('importance_mean', ascending=False)\n\nprint(\"   Top 10 features with permutation importance:\")\nprint(perm_df.head(10)[['feature', 'importance_mean', 'p_value', 'significant']].to_string(index=False))\n\n# 4. ROI Component Significance Tests\nprint(f\"\\n5. ROI COMPONENTS SIGNIFICANCE TESTS:\")\nroi_components = {\n    'Quality_Audience': df['Quality_Audience'],\n    'ER': df['ER'],\n    'Turkey': df['Turkey'],\n    'Comment_Rate': df['Comment_Rate'],\n    'Est_Post_Price': df['Est_Post_Price']\n}\n\nprint(\"   Individual component correlations with ROI_star:\")\nfor name, component in roi_components.items():\n    corr_coef, corr_p = stats.pearsonr(component, df['ROI_star'])\n    print(f\"   {name:15}: r={corr_coef:7.4f}, p={corr_p:.4e} {'***' if corr_p < 0.001 else '**' if corr_p < 0.01 else '*' if corr_p < 0.05 else ''}\")\n\n# 5. Model Validation Tests\nprint(f\"\\n6. MODEL VALIDATION TESTS:\")\n\n# Cross-validation significance test\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(reg, X_train, y_train, cv=5, scoring='r2')\ncv_mean, cv_std = cv_scores.mean(), cv_scores.std()\n\n# Test if CV scores are significantly > 0\nt_stat_cv, p_val_cv = stats.ttest_1samp(cv_scores, 0)\nprint(f\"   Cross-validation R² scores: {cv_mean:.4f} ± {cv_std:.4f}\")\nprint(f\"   T-test vs 0: t={t_stat_cv:.4f}, p={p_val_cv:.4e}\")\nprint(f\"   Model performance {'IS' if p_val_cv < 0.05 else 'IS NOT'} significantly better than baseline (α=0.05)\")\n\n# 6. Outlier Detection and Significance\nprint(f\"\\n7. OUTLIER ANALYSIS:\")\nz_scores = np.abs(stats.zscore(y))\noutliers = np.where(z_scores > 3)[0]\nprint(f\"   Number of outliers (|z-score| > 3): {len(outliers)} ({len(outliers)/len(y)*100:.1f}%)\")\n\nif len(outliers) > 0:\n    # Test if removing outliers significantly improves model\n    X_no_outliers = X.drop(outliers)\n    y_no_outliers = y.drop(outliers)\n    \n    reg_no_outliers = lgb.LGBMRegressor(\n        objective=\"regression\", metric=\"rmse\", learning_rate=0.03, \n        n_estimators=400, random_state=42, verbose=-1\n    )\n    \n    X_train_clean, X_val_clean, y_train_clean, y_val_clean = train_test_split(\n        X_no_outliers, y_no_outliers, test_size=0.2, random_state=42\n    )\n    \n    reg_no_outliers.fit(X_train_clean, y_train_clean)\n    r2_clean = reg_no_outliers.score(X_val_clean, y_val_clean)\n    r2_original = reg.score(X_val, y_val)\n    \n    print(f\"   R² with outliers: {r2_original:.4f}\")\n    print(f\"   R² without outliers: {r2_clean:.4f}\")\n    print(f\"   Improvement: {r2_clean - r2_original:.4f}\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY OF STATISTICAL SIGNIFICANCE\")\nprint(\"=\" * 60)\nprint(f\"• Model significantly outperforms random: {'✓' if t_p < 0.05 else '✗'}\")\nprint(f\"• Cross-validation significantly > 0: {'✓' if p_val_cv < 0.05 else '✗'}\")\nprint(f\"• Residuals are normally distributed: {'✓' if shapiro_p > 0.05 else '✗'}\")\nprint(f\"• Significant features found: {sum(perm_df['significant'])}/{len(perm_df)}\")\nprint(f\"• Significant ROI correlations: {sum(corr_df['significant'])}/{len(corr_df)}\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}